{
    // ===========================================================================
    // TOP-LEVEL METADATA
    // One document = one student's answer to one question, graded by 2+ models.
    // ===========================================================================
    "evaluation_id": "eval_cuboid_s123_q1_m2025_001",
    // Unique ID for this evaluation record (student + question + run).
    "schema_version": "1.0.0",
    // Version of this evaluation schema so you can evolve it over time.
    "created_at": "2025-03-01T10:30:00Z",
    // When this evaluation JSON was generated.
    "created_by": "automated_pipeline_v1",
    // Which pipeline/script/tool produced this evaluation.
    // ===========================================================================
    // CONTEXT: COURSE / ASSIGNMENT / QUESTION / RUBRIC HOOK
    // This ties the record to the educational context and the prompt/rubric used.
    // ===========================================================================
    "context": {
        "course_id": "cosc111_intro_programming",
        // Machine-friendly ID to group evaluations by course.
        "course_name": "COSC111: Introduction to Programming",
        // Human-readable course title (nice for dashboards / exports).
        "assignment_id": 5,
        // Assignment ID (integer)
        "assignment_title": "Cuboid Class and OOP Basics",
        // Human-readable assignment name.
        "question_source_path": "question_cuboid.md",
        // The path at which the question is in.
        "question_id": "q1_cuboid_class",
        // ID for this specific question within the assignment.
        "question_title": "Implementing a Cuboid class in Java",
        // Human-readable question title.
        "rubric_source_path": "rubric_cuboid.json"
        // Where that rubric came from (file path / URI) for traceability.
    },
    // ===========================================================================
    // STUDENT SUBMISSION
    // What the student actually submitted (code + metadata).
    // ===========================================================================
    "submission": {
        "student_id": "12345678",
        // Unique identifier for the student.
        "student_name": "Shlok Shah",
        "submitted_at": "2025-03-01T09:55:00Z",
        // Timestamp when this submission was received.
        "programming_language": "java"
        // Main language of the submission; used for routing to compilers/LLMs.
    },
    // ===========================================================================
    // RUBRIC SNAPSHOT
    // A copy of the rubric_cuboid.json info, with IDs for downstream linking.
    // ===========================================================================
    "rubric": {
        "rubric_id": "rubric_cuboid_v1",
        // Matches context.rubric_id; lets you merge or index on rubric.
        "title": "Cuboid OOP Rubric",
        // Human-readable rubric name.
        "total_points": 100,
        // Maximum score according to rubric (pulled from totalPoints in the rubric file).
        "categories": [
            // Categories mirror rubric_cuboid.json, but we add category_id for referencing.
            // TODO: This category_id thing needs to be a part of the actual rubric structure too.
            {
                "category_id": "syntax_compilation",
                // Stable ID used in category_scores and misconceptions.
                "name": "Syntax & Compilation",
                // Display name for this rubric category.
                "max_points": 40,
                // Max points for this category (from rubric_cuboid.json "points").
                "description": "Code compiles without errors, proper syntax and language conventions, appropriate use of language features relevant to Variables and Data Types, Functions/Methods, Input/Output Operations, Code Organization, Comments and Documentation, Problem Solving Approaches."
                // Detailed explanation of what this category is assessing.
            },
            {
                "category_id": "logic_correctness",
                "name": "Logic & Correctness",
                "max_points": 25,
                "description": "Algorithm correctness, proper implementation of requirements, handles edge cases appropriately, demonstrates understanding of Variables and Data Types, Functions/Methods, Input/Output Operations, Code Organization, Comments and Documentation, Problem Solving Approaches."
            },
            {
                "category_id": "output_functionality",
                "name": "Output & Functionality",
                "max_points": 20,
                "description": "Produces expected output format, meets all functional requirements, handles input/output correctly, demonstrates working solution."
            },
            {
                "category_id": "style_documentation",
                "name": "Style & Documentation",
                "max_points": 15,
                "description": "Clear variable and function names, proper indentation and formatting, appropriate comments, follows coding best practices."
            }
        ]
    },
    // ===========================================================================
    // MODELS
    // Per-model grading results. Keyed by model alias (supports 2+ models).
    // ===========================================================================
    "models": {
        // Each key is a model; add or remove models without changing schema.
        "gpt-5-nano": {
            // --- Model identity and run configuration --------------------------------
            "model_name": "gpt-5-nano-2025-08-07",
            // Human-readable model name with snapshot
            "provider": "openai",
            // Who provides this model (for analysis across vendors).
            "run_id": "run_2025_11_16_01",
            // ID of this model invocation for traceability in logs.
            "config": {
                "system_prompt_id": "grading_system_v3",
                // ID of system prompt template used.
                "rubric_prompt_id": "rubric_cuboid_prompt_v2"
                // ID of grading prompt template specific to this rubric/question.
            },
            // --- Aggregate score for this model --------------------------------------
            "scores": {
                "total_points_awarded": 85,
                // Sum of all category_scores[*].points_awarded.
                "max_points": 100,
                // Should match rubric.total_points. Duplicated for convenience.
                "percentage": 85.0
                // total_points_awarded / max_points * 100.
            },
            // --- Per-category rubric scores with justification -----------------------
            "category_scores": [
                {
                    "category_id": "syntax_compilation",
                    // Links to rubric.categories[*].category_id.
                    "category_name": "Syntax & Compilation",
                    // Redundant human-readable name.
                    "points_awarded": 38,
                    // Points this model gave in this category.
                    "max_points": 40,
                    // Max points for this category.
                    "reasoning": "Code compiles cleanly and follows Java syntax conventions with only minor stylistic issues.",
                    // Category-specific reasoning for the score.
                    "confidence": 0.95,
                    // Model's confidence in this category score (0-1).
                    "reasoning_tokens": 156
                    // Number of tokens in reasoning (proxy for explanation depth).
                },
                {
                    "category_id": "logic_correctness",
                    "category_name": "Logic & Correctness",
                    "points_awarded": 22,
                    "max_points": 25,
                    "reasoning": "Surface area and volume formulas are implemented correctly; no edge case handling needed for this task.",
                    "confidence": 0.90,
                    "reasoning_tokens": 142
                },
                {
                    "category_id": "output_functionality",
                    "category_name": "Output & Functionality",
                    "points_awarded": 15,
                    "max_points": 20,
                    "reasoning": "Output format is mostly correct, but the sample output requires additional labels or formatting.",
                    "confidence": 0.85,
                    "reasoning_tokens": 134
                },
                {
                    "category_id": "style_documentation",
                    "category_name": "Style & Documentation",
                    "points_awarded": 10,
                    "max_points": 15,
                    "reasoning": "Variable names are clear but there are no comments explaining the methods.",
                    "confidence": 0.88,
                    "reasoning_tokens": 98
                }
            ],
            // --- Human-readable feedback bundle for this model -----------------------
            "feedback": {
                "overall_comment": "Good implementation of the Cuboid class with correct constructors and methods. Minor issues with output formatting and lack of comments.",
                // Holistic comment which must be descriptive (what you'd show a student).
                "strengths": [
                    "Correct use of constructor overloading and 'this' keyword.",
                    "Accurate calculations for surface area and volume."
                ],
                // Bullet-style strengths.
                "areas_for_improvement": [
                    "Add comments to explain the purpose of each method.",
                    "Match the exact output format specified by the assignment."
                ]
                // Bullet-style weaknesses / next steps for the student.
            },
            // --- Misconceptions for this submission according to this model ----------
            "misconceptions": [
                {
                    "name": "Does not match required output format exactly",
                    // Human-readable label for this misconception.
                    "description": "The student prints information in a readable way but does not follow the exact output format specified in the problem, which may cause automated tests to fail.",
                    // Description of what behavior/understanding this misconception reflects.
                    "confidence": 0.92,
                    // Model's confidence (0–1) that this misconception truly applies.
                    "evidence": [
                        // Evidence shows EXACTLY where this misconception appears in the submission.
                        {
                            "source": "student_code",
                            // Where this snippet comes from: student_code | tests | text_answer | etc.
                            "file_path": "Cuboid.java",
                            // Which file the snippet lives in.
                            "language": "java",
                            // Language of the snippet (could also be "text" if it's a written explanation).
                            "snippet": "public void displayInfo() {\n    System.out.println(\"Color: \" + color);\n    System.out.println(\"Dimensions: \" + l + \" x \" + w + \" x \" + h);\n    System.out.println(\"Surface Area: \" + getSurfaceArea());\n    System.out.println(\"Volume: \" + getVolume());\n}\n",
                            // Concrete code that demonstrates the issue.
                            "line_start": 18,
                            "line_end": 24,
                            // Approximate line range in that file (for UI highlighting).
                            "note": "The assignment requires a specific output format as shown in the sample run; here, the labels and spacing differ."
                            // Concise explanation of why this snippet is considered evidence.
                        }
                    ],
                    "generated_by": "gpt-5-nano",
                    // Which model produced this misconception annotation.
                    "validated_by": null
                    // Optional human rater ID when a TA confirms this annotation.
                }
            ]
        },
        "gpt-oss-120b": {
            // Second model, same structure; lets you compare models on the same submission.
            "model_name": "gpt-oss-120b",
            "provider": "open_source_lab",
            "model_version": "2025-02-15",
            "run_id": "run_2025_03_01_02",
            "config": {
                "system_prompt_id": "grading_system_v3",
                "rubric_prompt_id": "rubric_cuboid_prompt_v2"
            },
            "scores": {
                "total_points_awarded": 78,
                "max_points": 100,
                "percentage": 78.0
            },
            "category_scores": [
                {
                    "category_id": "syntax_compilation",
                    "category_name": "Syntax & Compilation",
                    "points_awarded": 35,
                    "max_points": 40,
                    "reasoning": "Minor issues in formatting but no compilation errors.",
                    "confidence": 0.92,
                    "reasoning_tokens": 145
                },
                {
                    "category_id": "logic_correctness",
                    "category_name": "Logic & Correctness",
                    "points_awarded": 20,
                    "max_points": 25,
                    "reasoning": "Correct formulas but no explicit handling of invalid inputs.",
                    "confidence": 0.88,
                    "reasoning_tokens": 138
                },
                {
                    "category_id": "output_functionality",
                    "category_name": "Output & Functionality",
                    "points_awarded": 13,
                    "max_points": 20,
                    "reasoning": "Output order differs from the required sample and missing some labels.",
                    "confidence": 0.80,
                    "reasoning_tokens": 152
                },
                {
                    "category_id": "style_documentation",
                    "category_name": "Style & Documentation",
                    "points_awarded": 10,
                    "max_points": 15,
                    "reasoning": "Reasonable style but no Javadoc comments.",
                    "confidence": 0.85,
                    "reasoning_tokens": 102
                }
            ],
            "feedback": {
                "overall_comment": "Solid implementation overall, but test output alignment and documentation can be improved.",
                "strengths": [
                    "Correct use of object-oriented concepts for the Cuboid class."
                ],
                "areas_for_improvement": [
                    "Align printed output strictly with the assignment's sample format.",
                    "Add brief comments or Javadoc for public methods."
                ]
            },
            "misconceptions": [
                {
                    "name": "Does not match required output format exactly",
                    // Human-readable label for this misconception.
                    "description": "The student prints information in a readable way but does not follow the exact output format specified in the problem.",
                    // Description of what behavior/understanding this misconception reflects.
                    "confidence": 0.88,
                    // Model's confidence (0–1) that this misconception truly applies.
                    "evidence": [
                        // Evidence shows EXACTLY where this misconception appears in the submission.
                        {
                            "source": "student_code",
                            // Where this snippet comes from: student_code | tests | text_answer | etc.
                            "file_path": "Cuboid.java",
                            // Which file the snippet lives in.
                            "language": "java",
                            // Language of the snippet (could also be "text" if it's a written explanation).
                            "snippet": "System.out.println(\"Color: \" + color);\nSystem.out.println(\"Dimensions: \" + l + \" x \" + w + \" x \" + h);\n",
                            // Concrete code that demonstrates the issue.
                            "line_start": 19,
                            "line_end": 21,
                            // Approximate line range in that file (for UI highlighting).
                            "note": "Labels differ from those shown in the sample output."
                            // Concise explanation of why this snippet is considered evidence.
                        }
                    ],
                    "generated_by": "gpt-oss-120b",
                    // Which model produced this misconception annotation.
                    "validated_by": null
                    // Optional human rater ID when a TA confirms this annotation.
                }
            ]
        }
    },
    // ===========================================================================
    // COMPARISON
    // Computed by backend code after collecting all model outputs.
    // Extensible structure - add metrics as research needs evolve.
    // ===========================================================================
    "comparison": {
        // -----------------------------------------------------------------------
        // SCORE SUMMARY
        // Aggregate statistics across all grading models
        // -----------------------------------------------------------------------
        "score_summary": {
            "per_model_percentage": {
                "gpt-5-nano": 85.0,
                "gpt-oss-120b": 78.0
            },
            "mean": 81.5,
            // Arithmetic mean of all model scores
            "median": 81.5,
            // Median score (middle value; robust to outliers)
            "std_dev": 4.95,
            // Standard deviation (measure of spread)
            "variance": 24.5,
            // Variance (σ²)
            "coefficient_of_variation": 0.061,
            // CV = std_dev / mean
            // Measures relative variability (lower = more consistent grading)
            // Typical interpretation: <0.1 = low variance, 0.1-0.2 = moderate, >0.2 = high
            "min": 78.0,
            "max": 85.0,
            "range": 7.0
            // max - min (total spread)
        },
        // -----------------------------------------------------------------------
        // PAIRWISE COMPARISONS
        // Detailed model-vs-model differences (scales to N models)
        // -----------------------------------------------------------------------
        "pairwise_differences": [
            {
                "model_a": "gpt-5-nano",
                "model_b": "gpt-oss-120b",
                "total_points_diff": 7,
                // Raw point difference (model_a - model_b)
                "percentage_diff": 7.0,
                // Percentage point difference
                "absolute_percentage_diff": 7.0,
                // |percentage_diff| for threshold checks
                "category_differences": [
                    {
                        "category_id": "syntax_compilation",
                        "category_name": "Syntax & Compilation",
                        "model_a_points": 38,
                        "model_b_points": 35,
                        "difference": 3,
                        // Raw point difference
                        "percent_of_category_max": 7.5
                        // (difference / max_points) * 100
                        // Shows how significant the disagreement is for this category
                    },
                    {
                        "category_id": "logic_correctness",
                        "category_name": "Logic & Correctness",
                        "model_a_points": 22,
                        "model_b_points": 20,
                        "difference": 2,
                        "percent_of_category_max": 8.0
                    },
                    {
                        "category_id": "output_functionality",
                        "category_name": "Output & Functionality",
                        "model_a_points": 15,
                        "model_b_points": 13,
                        "difference": 2,
                        "percent_of_category_max": 10.0
                    },
                    {
                        "category_id": "style_documentation",
                        "category_name": "Style & Documentation",
                        "model_a_points": 10,
                        "model_b_points": 10,
                        "difference": 0,
                        "percent_of_category_max": 0.0
                    }
                ],
                "largest_category_disagreement": {
                    "category_id": "output_functionality",
                    "difference_percent": 10.0
                    // Highest percent_of_category_max among all categories
                }
            }
        ],
        // -----------------------------------------------------------------------
        // CATEGORY AGREEMENT ANALYSIS
        // Per-rubric-category statistics and agreement levels
        // -----------------------------------------------------------------------
        "category_agreement": [
            {
                "category_id": "syntax_compilation",
                "category_name": "Syntax & Compilation",
                "max_points": 40,
                "all_model_scores": {
                    "gpt-5-nano": 38,
                    "gpt-oss-120b": 35
                },
                "statistics": {
                    "mean": 36.5,
                    "median": 36.5,
                    "std_dev": 2.12,
                    "variance": 4.5,
                    "coefficient_of_variation": 0.058,
                    // CV for this category specifically
                    "range": 3
                },
                "agreement_level": "high",
                // Categorization based on CV or std_dev thresholds:
                // - "perfect": std_dev = 0 (all models gave same score)
                // - "high": CV < 0.15
                // - "medium": 0.15 ≤ CV < 0.30
                // - "low": CV ≥ 0.30
                "normalized_variance": 0.003,
                // variance / max_points²
                // Allows cross-category comparison (accounts for different max_points)
                "confidence_stats": {
                    "mean_confidence": 0.935,
                    "confidence_range": 0.03
                    // max - min confidence across models for this category
                }
            },
            {
                "category_id": "logic_correctness",
                "category_name": "Logic & Correctness",
                "max_points": 25,
                "all_model_scores": {
                    "gpt-5-nano": 22,
                    "gpt-oss-120b": 20
                },
                "statistics": {
                    "mean": 21.0,
                    "median": 21.0,
                    "std_dev": 1.41,
                    "variance": 2.0,
                    "coefficient_of_variation": 0.067,
                    "range": 2
                },
                "agreement_level": "high",
                "normalized_variance": 0.003,
                "confidence_stats": {
                    "mean_confidence": 0.89,
                    "confidence_range": 0.02
                }
            },
            {
                "category_id": "output_functionality",
                "category_name": "Output & Functionality",
                "max_points": 20,
                "all_model_scores": {
                    "gpt-5-nano": 15,
                    "gpt-oss-120b": 13
                },
                "statistics": {
                    "mean": 14.0,
                    "median": 14.0,
                    "std_dev": 1.41,
                    "variance": 2.0,
                    "coefficient_of_variation": 0.101,
                    "range": 2
                },
                "agreement_level": "medium",
                "normalized_variance": 0.005,
                "confidence_stats": {
                    "mean_confidence": 0.825,
                    "confidence_range": 0.05
                }
            },
            {
                "category_id": "style_documentation",
                "category_name": "Style & Documentation",
                "max_points": 15,
                "all_model_scores": {
                    "gpt-5-nano": 10,
                    "gpt-oss-120b": 10
                },
                "statistics": {
                    "mean": 10.0,
                    "median": 10.0,
                    "std_dev": 0.0,
                    "variance": 0.0,
                    "coefficient_of_variation": 0.0,
                    "range": 0
                },
                "agreement_level": "perfect",
                "normalized_variance": 0.0,
                "confidence_stats": {
                    "mean_confidence": 0.865,
                    "confidence_range": 0.03
                }
            }
        ],
        "category_insights": {
            "most_controversial": {
                "category_id": "output_functionality",
                "category_name": "Output & Functionality",
                "cv": 0.101,
                "reason": "Highest coefficient of variation among categories"
            },
            "most_agreed": {
                "category_id": "style_documentation",
                "category_name": "Style & Documentation",
                "cv": 0.0,
                "reason": "Perfect agreement across all models"
            },
            "lowest_confidence": {
                "category_id": "output_functionality",
                "mean_confidence": 0.825,
                "reason": "Models least confident in this category"
            }
        },
        // -----------------------------------------------------------------------
        // MISCONCEPTION ANALYSIS
        // Compare misconception detection across models
        // -----------------------------------------------------------------------
        "misconception_summary": {
            "total_by_model": {
                "gpt-5-nano": 1,
                "gpt-oss-120b": 1
            },
            "total_unique_misconceptions": 1,
            // Total distinct misconceptions across all models (after deduplication)
            "unique_to_single_model": 0,
            // Misconceptions found by only one model (potential blind spots or false positives)
            "consensus_misconceptions": 1,
            // Misconceptions identified by 2+ models (higher confidence)
            "consensus_ratio": 1.0,
            // consensus / total_unique
            // High ratio (>0.7) = models agree on what's wrong
            // Low ratio (<0.4) = models see different issues
            "average_misconceptions_per_model": 1.0,
            "misconception_overlap_matrix": {
                // Pairwise overlap (for N models, this scales)
                "gpt-5-nano_vs_gpt-oss-120b": {
                    "shared": 1,
                    "only_model_a": 0,
                    "only_model_b": 0,
                    "jaccard_similarity": 1.0
                    // shared / (model_a + model_b - shared)
                }
            }
        },
        // -----------------------------------------------------------------------
        // CONFIDENCE ANALYSIS
        // Examine model confidence patterns and confidence-score relationships
        // -----------------------------------------------------------------------
        "confidence_analysis": {
            "overall_misconception_confidence": {
                "mean": 0.90,
                "std_dev": 0.028,
                "min": 0.88,
                "max": 0.92
            },
            "per_category_confidence": {
                "syntax_compilation": {
                    "mean": 0.935,
                    "std_dev": 0.021
                },
                "logic_correctness": {
                    "mean": 0.89,
                    "std_dev": 0.014
                },
                "output_functionality": {
                    "mean": 0.825,
                    "std_dev": 0.035
                },
                "style_documentation": {
                    "mean": 0.865,
                    "std_dev": 0.021
                }
            },
            "confidence_score_correlation": 0.82,
            // Correlation between model confidence and scores awarded
            // High positive = models confident when giving high/low scores
            // Low/negative = confidence doesn't track with scores (interesting!)
            "high_confidence_disagreements": [
                // Cases where models are confident (>0.8) but disagree significantly
                {
                    "category_id": "output_functionality",
                    "model_confidences": {
                        "gpt-5-nano": 0.85,
                        "gpt-oss-120b": 0.80
                    },
                    "score_difference": 2,
                    "percent_difference": 10.0,
                    "flag": "confident_disagreement"
                    // Particularly interesting for research
                }
            ],
            "low_confidence_categories": [
                // Categories where any model has confidence < 0.7
                {
                    "category_id": "output_functionality",
                    "min_confidence": 0.80,
                    "models_with_low_confidence": []
                    // None in this example (threshold was 0.7)
                }
            ]
        },
        // -----------------------------------------------------------------------
        // MODEL CHARACTERISTICS
        // Understanding grading tendencies and behavior patterns
        // -----------------------------------------------------------------------
        "model_characteristics": {
            "strictness_ranking": [
                // Models ordered by average score given (lowest = strictest)
                {
                    "rank": 1,
                    "model": "gpt-oss-120b",
                    "average_score": 78.0,
                    "strictness_label": "strict",
                    // strict | moderate | lenient (based on deviation from mean)
                    "deviation_from_mean": -3.5
                },
                {
                    "rank": 2,
                    "model": "gpt-5-nano",
                    "average_score": 85.0,
                    "strictness_label": "lenient",
                    "deviation_from_mean": 3.5
                }
            ],
            "consistency_scores": {
                // Measure of how internally consistent each model is across categories
                "gpt-5-nano": {
                    "category_cv": 0.52,
                    // CV of category scores (higher = more variance across categories)
                    "label": "moderate_consistency"
                },
                "gpt-oss-120b": {
                    "category_cv": 0.48,
                    "label": "moderate_consistency"
                }
            },
            "misconception_detection_rate": {
                "gpt-5-nano": 1,
                "gpt-oss-120b": 1
            },
            "average_misconception_confidence": {
                "gpt-5-nano": 0.92,
                "gpt-oss-120b": 0.88
            },
            "reasoning_depth": {
                // Average tokens used in reasoning across categories
                "gpt-5-nano": 132.5,
                "gpt-oss-120b": 134.25
            }
        },
        // -----------------------------------------------------------------------
        // INTER-RATER RELIABILITY
        // Statistical measures of agreement (critical for publication)
        // -----------------------------------------------------------------------
        "reliability_metrics": {
            "pearson_correlation": 0.996,
            // Linear correlation between model scores across categories
            // Interpretation: >0.9 excellent, 0.7-0.9 strong, 0.4-0.7 moderate, <0.4 weak
            "spearman_correlation": 1.0,
            // Rank-order correlation (robust to non-linearity and outliers)
            "intraclass_correlation_icc": 0.973,
            // ICC(2,1) for absolute agreement
            // Interpretation: >0.90 excellent, 0.75-0.90 good, 0.50-0.75 moderate, <0.50 poor
            "cohens_kappa": null,
            // For categorical agreement (e.g., pass/fail); null if not applicable
            "krippendorff_alpha": 0.985,
            // General reliability across multiple raters/categories
            // Interpretation: >0.80 good reliability, 0.67-0.80 tentative, <0.67 discard
            "reliability_interpretation": "excellent",
            // Overall assessment: poor | fair | moderate | good | excellent
            "standard_error_of_measurement": 2.47,
            // SEM = std_dev * sqrt(1 - ICC)
            // Typical error in individual measurements
            "confidence_interval_95": {
                "lower": 76.56,
                "upper": 86.44
                // 95% CI for the true score: mean ± (1.96 * SEM)
            }
        },
        // -----------------------------------------------------------------------
        // ENSEMBLE DECISION
        // Final grade recommendation using ensemble strategies
        // -----------------------------------------------------------------------
        "ensemble_decision": {
            "recommended_score": 81.5,
            // Final recommended score
            "scoring_method": "mean",
            // How the score was computed: mean | median | weighted | trimmed_mean
            "alternative_scores": {
                "mean": 81.5,
                "median": 81.5,
                "weighted_mean": 82.1,
                // If using model-specific weights
                "trimmed_mean": 81.5
                // Mean after removing outliers (if ≥3 models)
            },
            "weights_used": {
                "gpt-5-nano": 0.6,
                "gpt-oss-120b": 0.4
            },
            "weighting_rationale": "historical_accuracy",
            // equal | historical_accuracy | confidence_based | performance_calibrated
            "confidence_in_decision": 0.87,
            // Based on model agreement (high agreement = high confidence)
            // Computed from: 1 - (CV or normalized_variance)
            "decision_uncertainty": 2.47,
            // Standard error of the ensemble decision
            "letter_grade": "B+",
            // If applicable (based on configurable thresholds)
            "pass_fail": "pass",
            // If applicable
            "consensus_level": "strong"
            // Qualitative assessment: strong | moderate | weak | divided
            // strong: CV < 0.10
            // moderate: 0.10 ≤ CV < 0.20
            // weak: 0.20 ≤ CV < 0.30
            // divided: CV ≥ 0.30
        },
        // -----------------------------------------------------------------------
        // ENSEMBLE QUALITY METRICS
        // Assess the value and effectiveness of this ensemble
        // -----------------------------------------------------------------------
        "ensemble_quality": {
            "diversity_score": 0.086,
            // Normalized measure of how different model assessments are
            // Computed from CV or variance
            // Sweet spot: 0.05-0.15 (enough diversity to add value, not so much to distrust)
            "redundancy_score": 1.0,
            // Proportion of misconceptions flagged by multiple models
            // High redundancy (>0.7) = reliable, consistent detection
            // Low redundancy (<0.4) = models catching different issues (complementary)
            "complementarity_score": 0.0,
            // Proportion of misconceptions unique to individual models
            // 1 - redundancy_score
            "overall_ensemble_value": "high",
            // Qualitative assessment: excellent | high | moderate | low | poor
            // Considers: diversity, redundancy, reliability, agreement
            "recommendation": "ensemble_reliable",
            // Action recommendation:
            // - ensemble_reliable: use ensemble grade confidently
            // - ensemble_adds_value: ensemble better than single model
            // - single_model_sufficient: models too similar
            // - needs_more_models: insufficient coverage
            // - too_divergent: models disagree too much
            "confidence_improvement_vs_single": 0.15
            // How much more confident we are with ensemble vs single model
            // Positive = ensemble reduces uncertainty
        },
        // -----------------------------------------------------------------------
        // FLAGS & RECOMMENDATIONS
        // Automated decision support for human review and research
        // -----------------------------------------------------------------------
        "flags": {
            "needs_human_review": false,
            // Boolean flag for escalation to TA/instructor
            "review_urgency": "none",
            // none | low | medium | high | critical
            "review_reasons": [],
            // Array of reasons (empty if no review needed)
            // Possible values:
            // - "large_score_gap": score_diff > threshold (e.g., 15%)
            // - "boundary_score": near pass/fail cutoff (e.g., within 5% of 50%)
            // - "low_confidence": any model confidence < threshold (e.g., 0.6)
            // - "high_confidence_disagreement": models confident but diverge
            // - "outlier_detected": one model deviates >2 std_devs
            // - "category_conflict": models disagree significantly on specific category
            // - "misconception_mismatch": models find completely different issues
            // - "low_reliability": ICC or correlation below acceptable threshold
            "overall_agreement": "high",
            // Qualitative summary: perfect | high | medium | low | conflicted
            // Based on CV, ICC, correlation metrics
            "interesting_for_research": true,
            // Flag unusual or valuable cases for deeper analysis
            "research_interest_reasons": [
                "high_inter_rater_reliability",
                "strong_model_agreement",
                "consensus_misconception_detection"
            ],
            // Why this case is interesting:
            // - high_inter_rater_reliability
            // - low_inter_rater_reliability
            // - strong_model_agreement
            // - strong_model_disagreement
            // - unique_misconception_pattern
            // - confidence_score_anomaly
            // - boundary_case
            // - outlier_model_behavior
            // - category_specific_patterns
            "recommended_actions": [
                "accept_ensemble_grade"
            ]
            // What should happen next:
            // - accept_ensemble_grade: high confidence, no issues
            // - review_recommended: moderate concerns
            // - review_required: significant issues
            // - investigate_model_calibration: systematic bias detected
            // - refine_rubric: models consistently struggle with category
            // - add_to_training_set: good example for model improvement
            // - flag_for_research: interesting patterns worth studying
        },
        // -----------------------------------------------------------------------
        // METADATA
        // Tracking and versioning
        // -----------------------------------------------------------------------
        "metadata": {
            "computed_at": "2025-03-01T10:31:00Z",
            "computation_version": "1.0.0",
            // Version of comparison algorithm/metrics
            "models_evaluated": [
                "gpt-5-nano",
                "gpt-oss-120b"
            ],
            "num_models": 2,
            "thresholds_config": {
                // Configurable thresholds used in flagging/categorization
                "human_review_score_diff_percent": 15.0,
                "boundary_score_margin_percent": 5.0,
                "low_confidence_threshold": 0.6,
                "high_confidence_threshold": 0.8,
                "outlier_std_devs": 2.0,
                "cv_high_agreement_max": 0.10,
                "cv_medium_agreement_max": 0.20,
                "icc_excellent_min": 0.90,
                "icc_good_min": 0.75
            }
        }
    }
}