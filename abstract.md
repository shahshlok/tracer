Large Language Models (LLMs) can fix and explain code. Effective tutoring often requires going deeper than syntantic and logic errors, it requires semantic understanding of code and how to think about writing code as opposed to writing error-prone code and fixing it later. Effective tutoring is a result of understanding why a novice wrote code a certain way, what could they be thinking about when writing code. We argue that evaluating such systems requires moving beyond correctness toward narrative fidelity: whether a model can reconstruct a plausible student belief from code alone.

