Explaining a bug is easy; understanding why a student wrote it is the real work of tutoring. As LLMs enter CS1 classrooms, the opportunity is not automated fixing, but helping instructors reason about students’ latent beliefs about execution—their notional machines. We argue LLMs should be treated as instructor-facing hypothesis generators, and belief attribution evaluated as risk-asymmetric: false diagnoses are more harmful than silence.

We argue evaluation practices for misconception diagnosis are misaligned with this goal. Recall-heavy metrics reward over-diagnosis, masking unsafe behavior when models attribute misconceptions to correct code. Diagnostic systems should instead demonstrate epistemic restraint: evidence-grounded narratives, calibrated abstention, and separation between what is structurally visible in code and what requires semantic inference about student intent.

To ground this critique, we report evidence from TRACER, a controlled testbed for evaluating belief attribution via label-blind semantic alignment between model-generated narratives and reference misconception descriptions. In 5-fold cross-validation on 1,200 LLM-generated Java CS1 submissions spanning 18 misconceptions, models achieve high recall (0.87) but only moderate precision (0.58), with most false positives arising from over-diagnosis of correct programs. A label-inclusive matching ablation saturates recall (0.98) while degrading specificity (0.77), illustrating how evaluation shortcuts inflate recall-based impressions of capability while worsening safety.

Based on these findings, we argue the community should reorient misconception-diagnosis research around three commitments: (1) treat specificity on correct code as a first-class metric; (2) stratify performance across the structural–semantic inference gap; and (3) pilot instructor-facing tools under standards that prioritize diagnostic humility over coverage.
