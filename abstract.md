Explaining a bug is easy; understanding why a student wrote it is the real work of tutoring. As LLMs enter CS1 classrooms, the opportunity is not just automated fixing, but helping instructors infer students’ latent beliefs about execution—their notional machines. We argue the community should explore LLMs as instructor-facing hypothesis generators—while treating belief attribution as risk-asymmetric: false attributions are significantly more harmful than silence.

This requires evaluation practices that reward diagnostic humility: evidence-grounded narratives and calibrated abstention on correct work. We further contend that diagnosis should be reported separately for structural misconceptions (visible in code) versus semantic misconceptions (requiring intent inference), rather than collapsed into a single score.

We present results from TRACER, a controlled testbed evaluating whether model-generated belief narratives align with reference misconception descriptions via label-blind semantic matching. In 5-fold cross-validation on 1,200 LLM-generated Java CS1 submissions spanning 18 misconceptions, LLMs achieve high recall (0.87) but only moderate precision (0.58); crucially, most false positives arise from over-diagnosis of correct code. A label-inclusive matching ablation can saturate recall (0.98) while degrading specificity (0.77), inflating recall-based impressions of capability while worsening safety.

We propose a research agenda: (1) establish specificity on correct code as a first-class metric; (2) stratify results by the structural–semantic gap; and (3) pilot instructor-facing tools under these standards on authentic student data.
