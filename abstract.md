**Abstract (Revised – Position Paper)**

Explaining a bug is easy; understanding *why a student wrote it* is the real work of tutoring. As large language models (LLMs) enter CS1 classrooms, the central opportunity is not automated fixing, but supporting instructors in reasoning about students’ latent beliefs about execution—their *notional machines*. We take a clear position: LLMs should be treated as **instructor-facing hypothesis generators**, and belief attribution must be evaluated as **risk-asymmetric**, where false diagnoses are substantially more harmful than silence.

We argue that prevailing evaluation practices for misconception diagnosis are misaligned with this goal. Metrics that privilege recall reward over-diagnosis, systematically masking unsafe behavior—especially when models attribute misconceptions to correct code. We contend that diagnostic systems must instead demonstrate *epistemic restraint*: evidence-grounded narratives, calibrated abstention, and explicit separation between what is structurally visible in code and what requires semantic inference about student intent.

To ground this critique, we report evidence from **TRACER**, a controlled testbed for evaluating belief attribution via label-blind semantic alignment between model-generated narratives and reference misconception descriptions. Across 1,200 LLM-generated Java CS1 submissions spanning 18 misconceptions, models achieve high recall (0.87) but only moderate precision (0.58), with most false positives arising from over-diagnosis of correct programs. An ablation using label-inclusive matching saturates recall (0.98) while further degrading specificity (0.77), illustrating how common evaluation shortcuts inflate apparent capability while worsening safety.

Based on these findings, we argue the community should reorient misconception-diagnosis research around three commitments: (1) treat specificity on correct code as a first-class metric; (2) stratify performance across the structural–semantic inference gap; and (3) pilot instructor-facing tools under standards that prioritize diagnostic humility over coverage.
