Large language models (LLMs) can fix and explain code, but tutoring requires diagnosing *why* novices wrote a program: the underlying incorrect reasoning captured by Notional Machines (student mental models) that plausibly generated it. We present TRACER, an evaluation framework for *narrative fidelity* that requires models to produce a “student belief” narrative from a CS1 submission and scores these open-ended outputs against reference misconception narratives using embedding similarity. This setup is label-blind for the model output: it does not require the model to output a misconception label/ID, and thresholds are calibrated via 5-fold stratified cross-validation. We evaluate on a controlled synthetic benchmark of 1,200 Java CS1 submissions with 18 seeded misconception types (275 seeded; 925 clean controls) validated by compilation and hidden tests. Across 3 LLM families and 4 prompting strategies, the main label-blind evaluation achieves precision 0.577 and recall 0.872 (F1=0.694) with specificity 0.848 on clean code; errors are dominated by false alarms on correct submissions. A label-aware ablation boosts recall to 0.982 but reduces specificity to 0.774, consistent with increased over-diagnosis when misconception identities are exposed. Performance varies substantially across Notional Machine categories, highlighting cases where correct diagnosis depends on intent weakly expressed in surface syntax. Simple agreement ensembles improve reliability, reaching F1=0.762 primarily through precision gains. We discuss implications for safer tutoring systems that treat diagnoses as hypotheses and outline requirements for validation on authentic student data.
