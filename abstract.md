Large language models (LLMs) can fix and explain code, but tutoring requires diagnosing *why* novices wrote a program: the underlying incorrect reasoning (mental model) that plausibly generated it. We present TRACER, an evaluation framework for *narrative fidelity* that requires models to produce a “student belief” narrative from a CS1 submission and scores it against ground-truth misconception narratives without using any labels, using embedding similarity with thresholds calibrated via 5-fold stratified cross-validation. To establish an upper bound with unambiguous intent labels, we construct a synthetic benchmark of 1,200 Java CS1 submissions with 18 seeded misconception types (275 seeded; 925 clean controls) validated by compilation and hidden tests. Across 3 LLM families and 4 prompting strategies, the main label-blind evaluation achieves precision 0.577 and recall 0.872 (F1=0.694) with specificity 0.848 on clean code; errors are dominated by false alarms on correct submissions. A label-aware ablation boosts recall to 0.982 but reduces specificity to 0.774, consistent with label leakage increasing over-diagnosis. Performance varies substantially across misconception categories, highlighting cases where correct diagnosis depends on intent weakly expressed in surface syntax. Simple agreement ensembles improve reliability, reaching F1=0.762 primarily through precision gains. We discuss implications for safer tutoring systems that treat diagnoses as hypotheses and outline requirements for validation on authentic student data.