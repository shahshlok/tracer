Explaining a bug is easy; understanding why a student wrote it is the real work of tutoring. As LLMs enter CS1 classrooms, the opportunity is not just automated fixing, but helping instructors infer students’ latent beliefs about execution (their notional machines). We argue the community should explore LLMs as instructor-facing hypothesis generators—while treating belief attribution as epistemically asymmetric: false attributions are significantly more harmful than silence.

This requires evaluation practices that reward diagnostic humility: evidence-grounded narratives and calibrated abstention on correct work. We further contend that diagnosis must be reported separately for structural misconceptions (visible in code) versus semantic misconceptions (requiring intent inference), rather than collapsed into a single score.

To demonstrate the urgency of these standards, we present data from TRACER, a controlled testbed evaluating whether model-generated belief narratives align with reference misconceptions. On 1,200 synthetic Java CS1 submissions spanning 18 misconceptions, LLMs achieve high recall (0.87) but only moderate precision (0.58); crucially, most false positives arise from the over-diagnosis of correct code. Ablation studies show how shortcut evaluation pipelines can artificially saturate recall (0.98) while degrading specificity (0.77), inflating apparent capability while worsening safety.

We propose a research agenda: (1) establish specificity on correct code as a first-class metric; (2) stratify results by the structural–semantic gap; and (3) pilot instructor-facing tools under these standards on authentic student data.