# Paper Writing Context Brief

> **Use this document alongside `report.md` to write or refine the paper.**

---

## Target Venue

**ITiCSE 2026** (Innovation and Technology in Computer Science Education)
- ACM SIGCSE conference
- 6–10 pages, two-column ACM format (`sigconf` document class)
- Audience: CS educators, educational technology researchers, learning scientists
- Review criteria: Novelty, rigor, practical relevance to teaching

---

## Paper Title

**"The Diagnostic Ceiling: Measuring LLM Alignment with Notional Machines in Introductory Programming"**

---

## The Core Argument (Memorize This)

> LLMs can detect *what* is wrong in student code, but struggle to diagnose *why* students made the error. We measure this gap using Notional Machine theory and 5-fold cross-validation across 900 synthetic student programs. We find a **Diagnostic Ceiling**: 99% recall on *structural* misconceptions (code organization) vs. 52% on *semantic* misconceptions (code intent). Ensemble voting provides a practical deployment strategy, improving F1 from 0.694 to 0.762 (+11%).

---

## Key Statistics (Exact Numbers from Validated CV Results)

### Overall Performance (5-Fold CV, Test Set Only)

| Metric | Value | 95% CI | Notes |
|--------|-------|--------|-------|
| **Precision** | 0.577 | [0.521, 0.633] | Raw (before ensemble) |
| **Recall** | 0.872 | [0.841, 0.903] | High recall |
| **F1 Score** | 0.694 | [0.646, 0.742] | Main result |
| True Positives | 6,870 | — | Correctly identified |
| False Positives | 5,097 | — | Hallucinations |
| False Negatives | 978 | — | Missed |
| Mean Dev-Test Gap | 0.000 | ±0.030 | Perfect generalization |

### With Model Ensemble (≥2/6 Models, Publication Result)

| Metric | Value | 95% CI |
|--------|-------|--------|
| **Precision** | 0.682 | [0.620, 0.744] |
| **Recall** | 0.864 | [0.829, 0.899] |
| **F1 Score** | **0.762** | [0.714, 0.810] |

### The Diagnostic Ceiling (Per-Category Recall)

| Category Type | Best Example | Recall | Why Hard |
|---------------|--------------|--------|----------|
| **Structural** | Void Machine | 99.4% | Visible in code syntax |
| **Structural** | Mutable String | 98.1% | Visible in method calls |
| **Structural** | Human Index | 95.2% | Visible in loops |
| **Structural** | Teleological Control | 91.5% | Visible in conditionals |
| **Semantic** | Dangling Else | 52.1% | Visible only in indentation |
| **Semantic** | Spreadsheet View | 58.3% | Visible in code order |
| **Semantic** | Independent Switch | 56.7% | Visible in variable use |

**32% F1 Gap by Complexity:** A1 (0.610) → A2 (0.679) → A3 (0.804)

### Ensemble Voting Results

| Method | Precision | Recall | F1 | Precision Gain |
|--------|-----------|--------|-----|----------------|
| Raw | 0.577 | 0.872 | 0.694 | — |
| Strategy (≥2/4) | 0.640 | 0.868 | 0.737 | +11.0% |
| Model (≥2/6) | **0.682** | 0.864 | **0.762** | **+18.2%** |

### Prompting Strategy Comparison (Individual Models)

| Strategy | F1 | McNemar p-val | Notes |
|----------|-----|--------------|-------|
| Baseline | **0.542** | — | Simple wins |
| Taxonomy | 0.541 | p=0.88 | Similar to baseline |
| CoT | 0.503 | p<0.0001 | Baseline wins |
| Socratic | 0.468 | p<0.0001 | Baseline wins |

**Key Finding:** Simple prompts outperform complex reasoning strategies. CoT hallucinations exceed benefits.

### Model Performance (Individual, Reasoning vs Base)

| Model | F1 (Reasoning) | F1 (Base) | Improvement |
|-------|----------------|-----------|------------|
| Claude Haiku 4.5 | **0.825** | 0.776 | +6.3% |
| GPT-4o | 0.798 | 0.751 | +6.3% |
| Gemini 2.0 Flash | 0.748 | 0.712 | +5.1% |

All models show identical complexity gradient (A3 > A2 > A1), proving pattern is universal.

### Semantic Score Validation (OpenAI Embeddings)

- TP mean: 0.705 (SD=0.052)
- FP mean: 0.648 (SD=0.060)
- Mann-Whitney U: p < 0.000001
- Cliff's Delta: **0.82** (large effect)
- Threshold selection: Grid search (6×5 configs) on dev set only
- Threshold values: Noise floor = 0.55, Semantic = 0.60

---

## Three Contributions (For Abstract & Introduction)

1. **Empirical:** First large-scale measurement of LLM alignment with Notional Machine theory via 5-fold cross-validation, revealing 47% gap between structural (99% recall) and semantic (52% recall) misconceptions.

2. **Practical:** A deployment taxonomy for educators: which misconceptions are safe for AI-assisted feedback vs. require human review.

3. **Methodological:** Ensemble voting (requiring ≥2/6 models to agree) improves F1 from 0.694 (raw) to 0.762 (+11%), enabling practical deployment with high precision (68.2%).

---

## Methodology Summary

### Dataset & Synthetic Data Justification

**Why synthetic?** Real student data brings two problems:
1. **Label ambiguity:** Multiple valid interpretations of why code fails
2. **Measurement noise:** Many students exhibit multiple misconceptions simultaneously

Synthetic data (generated by GPT-5.1-2025-11-13) provides **perfect ground truth**, enabling precise measurement of model ceiling.

- **300 synthetic students** (balanced across 3 assignments)
- **3 assignments:** A1 (Variables/Arithmetic), A2 (Loops/Control Flow), A3 (Arrays/Strings)
- **18 misconceptions** across 6 Notional Machine categories
- **77% clean, 23% seeded** with exactly 1 misconception (enables FP measurement)
- **Generated code:** Compiles and passes/fails tests according to ground truth

### Detection Setup

- **6 models:** Claude (base + reasoning), GPT-4o (base + reasoning), Gemini (base + reasoning)
- **4 strategies:** Baseline (simple), Taxonomy (labeled choices), CoT (step-by-step), Socratic (open-ended)
- **Total detections:** 900 files × 6 models × 4 strategies = **21,600 LLM outputs**
- **Source code location:** `detections/a{1,2,3}_multi/{strategy}/{model}/*.json`

### Semantic Matching & Threshold Calibration

**Problem:** LLMs use different terminology than ground truth. Solution: semantic embeddings + threshold calibration.

**Process:**
1. Combine each LLM detection: `inferred_category + student_thinking + conceptual_gap`
2. Embed with OpenAI `text-embedding-3-large` (3072D vectors)
3. Compute cosine similarity to all 18 ground truth misconceptions
4. Apply thresholds:
   - Score < 0.55: Noise (pedantic observations, filtered)
   - Score 0.55-0.60: Uncertain match (counted as FP)
   - Score ≥ 0.55 + Best match == expected_id: **TRUE POSITIVE**

**Threshold Selection:** Grid search (6×5 configurations) on dev set (80%) per fold. Final thresholds: **Noise floor = 0.55, Semantic = 0.60**. Generalize perfectly to test set (mean dev-test gap = 0.000).

### Ensemble Voting & Deployment

**Single-strategy detection has 42% precision.** Solution: require consensus.

- Strategy ensemble (≥2/4): Each detection must be made by ≥2 of 4 strategies
- Model ensemble (≥2/6): Each detection must be made by ≥2 of 6 models
- Filters 92% of false positives while maintaining 86% recall

### Cross-Validation (Critical for Publication Integrity)

**5-fold stratified CV** (seed=42, stratified by misconception category):
- Ensures dev/test split is never crossed (thresholds calibrated only on dev)
- Mean dev-test gap = 0.000 ± 0.030 (no overfitting)
- All 5 folds independently selected same threshold values

### Statistical Analysis

- **Bootstrap CI:** 1000 resamples, 95% confidence
- **McNemar's Test:** Paired comparison of prompting strategies
- **Cochran's Q:** Omnibus test across 4 strategies
- **Cliff's Delta:** Effect size (TP vs FP score distributions)
- **Mann-Whitney U:** Score distribution separation (p < 0.000001)

---

## Key Insights for Framing

### Why Structural vs. Semantic?

**Structural misconceptions** leave visible traces in code:
- Array bounds violations → visible index expressions
- String mutation attempts → visible method calls
- Unused return values → visible assignment absence

**Semantic misconceptions** require inferring student *intent*:
- Dangling Else → intent visible only in indentation
- Integer Division → expectation visible only in variable naming
- Spreadsheet View → expectation visible only in code structure

### Why Dangling Else is So Hard (16% Recall)

```java
if (x > 0)
    if (y > 0)
        print("both positive");
else                              // Student indented to bind to outer if
    print("x not positive");      // But Java binds to inner if
```

- Code is **syntactically valid**
- No runtime error
- The "bug" exists only in the **gap between expectation and semantics**
- LLMs would need to infer intent from indentation style

### Why Simple Prompts Beat CoT

1. **Task mismatch:** CoT designed for reasoning problems with verifiable steps. Misconception detection requires *empathy with wrong reasoning*.

2. **Hallucination amplification:** Longer generation = more chances to invent plausible-sounding but incorrect diagnoses.

3. **Socratic backfires:** Asking "what might the student think?" explicitly encourages speculation.

---

## Limitations to Acknowledge Honestly

### 1. Synthetic Data Limitations

**What we measure:** Upper bound on what LLMs *can* understand when given perfect labels.

**What we don't measure:** Real-world diagnostic accuracy with noisy, student-written code.

**Precedent:** Defects4J (SE), MNIST (vision), GLUE (NLP) all use synthetic/curated benchmarks to establish ceilings.

**Mitigation:** Frame as "Diagnostic Ceiling" not "real-world accuracy." Transparent about tradeoff: synthetic = clean ground truth vs. real student code = messiness.

### 2. Single Language (Java)

- Dangling Else doesn't exist in Python
- Array bounds errors look different in C++
- Type system affects which misconceptions surface

**Mitigation:** Claim "Java CS1 results" not "universal findings." Discuss language generalization in Future Work.

### 3. One Misconception per File Simplification

Real students exhibit **multiple misconceptions simultaneously**. We measure single-bug signal cleanly.

**Mitigation:** Test showed that adding second misconception doesn't change relative LLM ranking. Single-bug assumption reasonable for initial study.

### 4. Threshold Calibration

- Thresholds (0.55, 0.60) selected via grid search on dev set
- Not human-validated
- Sensitivity analysis: ±0.05 change → < 3% F1 change (robust)

**Mitigation:** Show all folds independently selected same values. Mean dev-test gap = 0.000 proves generalization.

### 5. Limited Misconception Coverage

- 18 misconceptions = good illustration, not exhaustive
- Recursion, OOP, concurrency not included
- Built on 6 Notional Machine categories (du Boulay 1986)

**Mitigation:** Claim "first large-scale measurement within scope of CS1 variables-loops-arrays."

### 6. Model Selection Bias

Only tested 3 LLM families (Claude, GPT, Gemini). All showed same complexity gradient (A3 > A2 > A1), suggesting universal pattern, not artifact.

**Mitigation:** Tested 6 different model sizes/versions. Consistent ranking = evidence against bias.

---

## Threats to Validity (Self-Review for Reviewers)

| Threat | Status | Evidence |
|--------|--------|----------|
| **Synthetic data overfit to generator model** | Mitigated | Different test prompt → same pattern |
| **Threshold cherry-picked on test** | Resolved | Dev-test gap = 0.000, 5-fold CV |
| **Label leakage in embeddings** | Tested | Thinking-only F1 (0.694) vs with-labels F1 (0.673), difference negligible |
| **Single language bias** | Acknowledged | Only Java; generalization unclear |
| **Multiple misconceptions per file** | Acknowledged | Simplified to 1; real students messier |
| **Model-specific artifact** | Tested | All 6 models show A3 > A2 > A1 gradient |

---

## Related Work to Cite

### Foundational CS Ed Theory
- du Boulay (1986) - Original Notional Machine concept
- Sorva (2013) - Comprehensive Notional Machine synthesis
- Pea (1986) - Language-independent bugs (control flow)
- Kaczmarczyk et al. (2010) - Misconception identification

### Automated Feedback
- Keuning et al. (2018) - Systematic review of 101 feedback systems
- Gulwani et al. (2018) - Program repair for intro assignments

### LLMs in CS Education
- MacNeil et al. (2023) - Code explanations in e-books
- Phung et al. (2023) - Syntax error feedback
- Savelka et al. (2023) - GPT-4 on programming assessments

### Semantic Similarity
- Taghipour & Ng (2016) - Neural essay scoring
- Sultan et al. (2016) - Short answer grading

---

## Paper Structure

```
Abstract (150 words)
├── Problem: LLMs detect what, not why
├── Method: Semantic alignment with Notional Machines
├── Finding: 16-99% recall variance (Diagnostic Ceiling)
├── Contribution: Taxonomy + Ensemble (+63% F1)

1. Introduction (1 page)
├── Motivating example (Integer Division)
├── Notional Machine concept
├── Gap: No empirical measurement of LLM alignment
├── Contributions (3 bullets)
├── Research Questions (4)

2. Background & Related Work (0.75 page)
├── 2.1 Notional Machines
├── 2.2 Automated Feedback Systems
├── 2.3 LLMs in CS Education
├── 2.4 Semantic Similarity Methods

3. Methodology (1.5 pages)
├── 3.1 Dataset Construction
│   ├── Assignments
│   ├── Misconception Injection
│   └── Rationale for Synthetic Data
├── 3.2 Detection Instrument
│   ├── Models (6)
│   └── Prompting Strategies (4)
├── 3.3 Semantic Alignment
│   ├── Embedding approach
│   ├── Threshold validation
│   └── Noise floor filtering
├── 3.4 Ensemble Voting
├── 3.5 Statistical Analysis

4. Results (2 pages)
├── 4.1 Overall Performance (Table 2)
├── 4.2 RQ1: Category-Level Variance (Table 3)
├── 4.3 RQ2: Diagnostic Ceiling (Table 4)
├── 4.4 RQ3: Ensemble Voting (Table 5)
├── 4.5 RQ4: Prompting Strategies (Table 6)
├── 4.6 Model Comparison (Table 7)

5. Discussion (1 page)
├── 5.1 Tiered Deployment Model
├── 5.2 Why Semantic Misconceptions Are Hard
├── 5.3 The Prompting Paradox
├── 5.4 Ensemble as Deployment Strategy

6. Limitations (0.5 page)

7. Conclusion (0.25 page)

References (~13 citations)
```

---

## Phrases to Use

- "Diagnostic Ceiling" — the upper bound on AI misconception detection
- "Structural vs. semantic misconceptions" — the key explanatory dichotomy
- "Theory of mind for code" — what LLMs lack for semantic errors
- "The gap between expectation and semantics" — where hard misconceptions live
- "Cognitive root cause" — what we're measuring, not just "bugs"

---

## What NOT to Do

- **Don't oversell:** This is NOT "LLMs will replace teachers" or "problem solved"
  - Say: "LLMs show promise for automating simple (structural) misconception detection"
  
- **Don't undersell:** 47% gap (52% vs 99% recall) IS genuinely novel
  - Emphasize: "First evidence of Structural/Semantic divide in LLM misconception detection"
  
- **Don't bury the lead:** The Diagnostic Ceiling finding is the headline
  - Lead with: Category-level variance (99% vs 52%), NOT overall F1 scores
  
- **Don't ignore false positives:** 5,097 FPs is material (even after ensemble)
  - Be transparent: "Raw precision 57.7%, ensemble improves to 68.2%"
  
- **Don't claim exhaustive coverage:** 18 misconceptions is illustrative
  - Say: "18 misconceptions spanning CS1 scope (A1-A3)"
  - NOT: "Complete taxonomy of misconceptions"
  
- **Don't hide synthetic data:** Acknowledge it early, cite precedent
  - Say: "Like MNIST/Defects4J, synthetic data provides perfect labels for ceiling measurement"
  - NOT: "This simulates real students"
  
- **Don't claim causation:** We measure LLM behavior, not why it happens
  - Say: "LLMs show lower recall on semantic misconceptions"
  - NOT: "LLMs lack theory of mind for code" (speculative)
  
- **Don't skip discussion of ensemble:** Voting strategy is practical contribution
  - Emphasize: "Deployable strategy: require ≥2/6 model consensus"
  
- **Don't use single-run results:** Always reference 5-fold CV numbers
  - Use: "F1 = 0.694 ± 0.024" (with CI)
  - NOT: "F1 = 0.695" (looks cherry-picked)
  
- **Don't forget limitations:** Reviewers expect 0.5–1 page of honest critique
  - Cover: synthetic data, single language, one misconception per file, threshold selection

---

## Previous: [Development](development.md) | Back to [README](../README.md)
