# Dataset Generation

This document explains how the synthetic student dataset is generated. This is critical context for understanding the validity of experimental results.

---

## Important: The Data is 100% Synthetic

The `authentic_seeded/` directory contains **entirely LLM-generated code**, not submissions from real students.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              DATA HONESTY NOTE                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  The directory is named "authentic_seeded" but this is a MISNOMER.          │
│                                                                             │
│  - "Students" are fake identities generated by Faker                        │
│  - Code is written by GPT-5.1 with persona prompts                          │
│  - Misconceptions are injected by prompting the LLM                         │
│                                                                             │
│  This is intentional: we need KNOWN ground truth for measurement.           │
│  Real student data would have ambiguous or unknown misconceptions.          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Why Synthetic Data?

| Real Student Data | Synthetic Data |
|-------------------|----------------|
| Unknown ground truth | Known misconception ID per file |
| Ambiguous errors | One clean misconception per file |
| Privacy concerns | No ethical issues |
| Hard to scale | Generate any sample size |
| Natural distribution | Controlled distribution |

The trade-off: synthetic bugs may not perfectly match real student error patterns, but we gain **measurement validity** through known labels.

---

## The 6-Step Pipeline

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                         DATASET GENERATION PIPELINE                           │
└──────────────────────────────────────────────────────────────────────────────┘

                              Generator Model: gpt-5.1-2025-11-13

  STEP 1                STEP 2              STEP 3
  Generate              Compile             Test
  Correct Code          Check               Check
  ─────────────         ────────            ────────
  ┌───────────┐        ┌───────────┐       ┌───────────┐
  │ GPT-5.1   │───────▶│  javac    │──────▶│ Run tests │
  │ + Persona │        │  compile  │       │ All pass? │
  └───────────┘        └───────────┘       └───────────┘
        │                    │                   │
        │ Retry 3x           │ Retry 3x          │ Retry 3x
        │ if fails           │ if fails          │ if fails
        ▼                    ▼                   ▼

  STEP 4                STEP 5              STEP 6
  Inject                Compile             Test
  Misconception         Check               Check
  ─────────────         ────────            ────────
  ┌───────────┐        ┌───────────┐       ┌───────────┐
  │ GPT-5.1   │───────▶│  javac    │──────▶│ Run tests │
  │ + Error   │        │  compile  │       │ ≥1 fail?  │
  └───────────┘        └───────────┘       └───────────┘
        │                    │                   │
        │ Retry 3x           │ Retry 3x          │ Must fail
        │ if fails           │ if fails          │ AND differ
        ▼                    ▼                   ▼

                        ┌───────────┐
                        │  OUTPUT   │
                        │ 4 files:  │
                        │ 3 clean   │
                        │ 1 seeded  │
                        └───────────┘
```

### Step Details

| Step | Input | Validation | On Failure |
|------|-------|------------|------------|
| 1. Generate Correct | Question + Persona | - | Retry 3x, then discard student |
| 2. Compile Correct | Java source | `javac` succeeds | Retry step 1 |
| 3. Test Correct | Compiled class | All test cases pass | Retry step 1 |
| 4. Inject Misconception | Correct code + error spec | - | Retry 3x |
| 5. Compile Seeded | Java source | `javac` succeeds | Retry step 4 |
| 6. Test Seeded | Compiled class | ≥1 test fails AND differs from correct | Retry step 4, fallback to clean |

**Note on testing:** TRACER’s generator uses a lightweight **black-box I/O harness**: it compiles each submission and executes `main` on fixed stdin test cases, then checks stdout for expected output patterns (plus optional forbidden patterns).

---

## The Persona Matrix

To create surface-level variation (so detectors can't overfit to style), we use a 4×3 matrix:

### Coding Styles (4)

| Style ID | Description |
|----------|-------------|
| `style_minimal` | Single-letter variables, no comments, one-liners |
| `style_verbose` | Long descriptive names, comments explaining each step |
| `style_textbook` | Clean standard Java, camelCase, 4-space indent |
| `style_messy` | Inconsistent indentation, mixed naming conventions |

### Cognitive Profiles (3)

| Profile ID | Description |
|------------|-------------|
| `cog_procedural` | Code like a recipe: step 1, step 2, done |
| `cog_mathematical` | Structure around formulas, intermediate variables |
| `cog_cautious` | Explicit edge case checks, temporary holders |

### Combined Persona Prompt

```
You are a CS1 student writing Java code.
Use single-letter variable names (x, y, n). No comments. Prefer one-liners.
You think about code like following a recipe. Do step 1, then step 2, then done.
```

This produces 12 distinct "student types" per assignment.

---

## Misconception Injection

Each misconception in `groundtruth.json` includes:

```json
{
  "id": "NM_STATE_01",
  "category": "The Reactive State Machine",
  "name": "Spreadsheet View (Early Calculation)",
  "student_thinking": "Variables update automatically when sources change",
  "instructions_for_llm": {
    "Q1": "Initialize v0=0, v1=0, t=0. Compute double a = (v1-v0)/t; BEFORE reading input..."
  }
}
```

The `instructions_for_llm` field provides **explicit injection instructions** per question.

### Injection Prompt Structure

```
Here is a correct solution:
[correct code]

Your Task: Rewrite as a student with the 'Reactive State Machine' misconception.

Your Mental Model:
The student views variables as reactive constraints (like Excel cells)...

Your Internal Monologue:
"I'll define the formula at the top so it's ready..."

Action Plan:
Initialize v0=0, v1=0, t=0. Compute double a = (v1-v0)/t; BEFORE reading input...

Instructions:
- You believe your code is correct
- Keep original coding style
- The code MUST compile
- Do NOT add comments explaining the error
```

---

## Output Structure

For each synthetic student:

```
authentic_seeded/a1/
├── Smith_John_123456/
│   ├── Q1.java    ← SEEDED (has misconception)
│   ├── Q2.java    ← Clean (correct code)
│   ├── Q3.java    ← Clean (correct code)
│   └── Q4.java    ← Clean (correct code)
```

**Important:** Each student has exactly **1 seeded file** and **3 clean files**. This ratio enables false positive measurement.

---

## Pipeline Statistics

The generator tracks validation failures:

```json
{
  "correct_compile_failures": 12,
  "correct_test_failures": 8,
  "seeded_compile_failures": 45,
  "seeded_test_pass_failures": 23,
  "seeded_no_diff_failures": 5,
  "seeded_fallback_to_clean": 18,
  "successful_samples": 82,
  "discarded_samples": 18
}
```

| Metric | Meaning |
|--------|---------|
| `seeded_fallback_to_clean` | Students where seeding failed; got 4 clean files instead |
| `seeded_test_pass_failures` | Seeded code passed all tests (bug not actually injected) |
| `seeded_no_diff_failures` | Seeded code identical to correct (no change made) |

---

## Running the Generator

```bash
# Generate 10 students for A3
uv run python -m utils.generators.dataset_generator generate \
    --assignment a3 \
    --students 10 \
    --model gpt-5.1-2025-11-13 \
    --output authentic_seeded/a3 \
    --seed 12345

# View pipeline stats
cat authentic_seeded/a3/pipeline_stats.json
```

### CLI Options

| Option | Default | Description |
|--------|---------|-------------|
| `--assignment` | `a3` | Which assignment (a1, a2, a3) |
| `--students` | `10` | Number of students to generate |
| `--model` | `gpt-5.1-2025-11-13` | OpenAI model for generation |
| `--output` | `authentic_seeded/a3` | Output directory |
| `--seed` | timestamp | Random seed for reproducibility |

---

## Limitations

(See also `docs/synthetic-validation-audit.md` for an audit of validation behavior verified during development.)

### 1. LLM-Injected Bugs May Not Match Reality

Real students make errors through genuine confusion. LLMs follow explicit instructions. The resulting code may be:
- Too clean (real bugs are messier)
- Too consistent (real bugs vary more)
- Too literal (follows instructions exactly)

### 2. Persona Matrix is Shallow

Style variation (indentation, naming) doesn't capture:
- True cognitive differences
- Background knowledge gaps
- Debugging behaviors

### 3. Same Generator for All Students

All code comes from one model checkpoint. Real cohorts have diverse backgrounds.

### 4. One Bug Per File Constraint

Real student code often has multiple interacting errors. We isolate for measurement clarity.

---

## Reproducibility

To reproduce the exact dataset used in this research:

```bash
# The final dataset was generated with:
uv run python -m utils.generators.dataset_generator generate \
    --assignment a1 --students 100 --seed 1765036611
uv run python -m utils.generators.dataset_generator generate \
    --assignment a2 --students 100 --seed 1765128549
uv run python -m utils.generators.dataset_generator generate \
    --assignment a3 --students 100 --seed 1765142264
```

Note: Exact reproducibility requires the same model checkpoint, which may not be available after model updates.

---

## Previous: [Architecture](architecture.md) | Next: [Notional Machines](notional-machines.md)
