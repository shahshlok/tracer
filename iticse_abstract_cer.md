# ITiCSE Computing Education Research (CER) Track Abstract

Large language models (LLMs) can fix and explain code, but tutoring requires diagnosing why novices wrote it: the studentâ€™s underlying notional machine. We ask: (RQ1) Can LLMs infer student beliefs from CS1 submissions? (RQ2) How often do they over-diagnose misconceptions in correct code? We introduce TRACER, a narrative-fidelity evaluation framework: given a submission, a model produces a student-belief narrative; we score it by semantic similarity to reference misconception narratives, with thresholds tuned by 5-fold stratified cross-validation. We evaluate on a controlled synthetic benchmark of 1,200 Java CS1 programs (18 seeded misconception types; 275 seeded, 925 clean controls) validated by compilation and hidden tests. Across three LLM families and four prompting strategies, label-blind narrative scoring achieves precision 0.577, recall 0.872 (F1=0.694) and specificity 0.848; errors are dominated by false positives on clean submissions. A label-aware ablation raises recall to 0.982 but reduces specificity to 0.774, consistent with increased over-diagnosis when misconception identities are exposed. Performance varies across notional machine categories, suggesting that some misconceptions are weakly expressed in surface syntax. Simple agreement ensembles improve reliability (best F1=0.762) primarily via precision gains. We discuss implications for safer tutoring systems that treat diagnoses as hypotheses and outline validation requirements for authentic student data.

