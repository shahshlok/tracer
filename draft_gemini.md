# Simulating the Socratic Tutor: Measuring the Narrative Fidelity of LLM Diagnostics on Notional Machine Misconceptions

## Abstract
As Large Language Models (LLMs) are increasingly integrated into introductory programming (CS1) classrooms, research has predominantly focused on their ability to generate code or fix syntax errors. However, effective pedagogy requires diagnosing the underlying *notional machine misconceptions*—the flawed mental models of computer state—that drive student errors. This study evaluates the "Cognitive Alignment" of three LLM families (GPT-5.2, Claude-Haiku-4.5, Gemini-3-Flash) in a controlled, synthetic environment. Using a dataset of 1,200 procedural Java submissions generated by simulated student agents with injected misconceptions, we measure **Narrative Fidelity**: the ability of models to articulate *why* a student made an error without being primed with the answer. We find that enforcing this narrative requirement acts as a crucial calibration mechanism: while a label-matching baseline achieves higher recall (0.982) by aggressively flagging errors, it suffers from poor precision (0.511). In contrast, requiring models to reconstruct the student's thought process improves precision to 0.577 and specificity to 84.8% (F1=0.694), effectively filtering hallucinations. We further identify a "Visibility Bias": models excel at diagnosing structural errors (F1>0.95) but struggle with invisible state misconceptions (F1<0.75), suggesting that current LLMs remain syntax-biased observers. These findings establish an upper bound on LLM diagnostic capability and highlight the necessity of "reasoning-first" architectures for AI tutors.

## 1. Introduction
The integration of AI into Computer Science Education (CSEd) has largely been functionalist: *Can the tool fix the bug?* (Kazemitabaar et al., 2023). Yet, constructivist pedagogy suggests that errors are not merely broken code but symptoms of consistent, albeit incorrect, mental models known as "Notional Machines" (Sorva, 2013). A student who attempts to swap variables without a temporary variable isn't just making a syntax error; they are operating under a "fluid state" model where assignment works like algebraic equality.

For an AI tutor to be pedagogically effective, it must do more than patch code; it must diagnose the student's intent. This raises a critical question: Can LLMs infer the invisible thought processes of a novice programmer?

This paper presents **TRACER** (Taxonomic Research of Aligned Cognitive Error Recognition), a framework to evaluate this capability. We depart from standard "bug classification" tasks by measuring **Narrative Fidelity**—the degree to which an LLM can reconstruct the semantic "story" of a misconception.

We investigate three research questions:
*   **RQ1 (The Reasoning Filter):** Does requiring an LLM to articulate a student's thought process improve diagnostic reliability compared to simple label matching?
*   **RQ2 (The Visibility Gap):** Does diagnostic performance degrade as misconceptions shift from syntax-visible (structural) to logic-invisible (semantic) errors?
*   **RQ3 (Diagnostic Reliability):** Can ensemble methods mitigate the risk of "pedagogical hallucinations" in automated tutoring?

## 2. Methodology
To isolate diagnostic capability from the noise of real-world classroom data, we utilized a **Simulated Student Proxy** methodology. This allows us to establish a "ground truth" of student intent that is often ambiguous in naturalistic datasets.

### 2.1 The Synthetic Sandbox
We generated a controlled dataset of 1,200 Java submissions using a 6-step validation pipeline. We modeled 12 distinct student personas (varying in coding style and cognitive profile) and injected 18 specific Notional Machine misconceptions drawn from CSEd literature (Sorva, 2013).
*   **The Artifacts:** 300 synthetic students × 4 questions (Assignments: A1 Variables, A2 Control Flow, A3 Arrays).
*   **The Injection:** 275 files contain verified misconceptions (e.g., *The Reactive State Machine*, *The Human Index Machine*); 925 files are clean controls to measure false alarm rates.
*   **Validity Note:** While synthetic data limits ecological validity, it provides a "clean room" to measure the model's maximum theoretical capability (an upper bound). If models fail here, they will likely fail in the wild.

### 2.2 The TRACER Evaluation Framework
We evaluated three model families (GPT-5.2, Claude-Haiku-4.5, Gemini-3-Flash) using four prompting strategies: *Baseline* (detect error), *Taxonomy* (definitions provided), *Chain-of-Thought* (trace execution), and *Socratic* (probe intent).

We introduce a **Label-Blind Semantic Metric**. Standard string matching fails on open-ended reasoning. Instead, we embed the LLM's generated "Student Thought Process" using `text-embedding-3-large` and measure cosine similarity against the ground-truth misconception narrative.
*   **Main Protocol (Thinking-Only):** The evaluation is performed without seeing the misconception label, matching only on the *narrative description*.
*   **Ablation Protocol (Thinking+Labels):** We verify the impact of the narrative requirement by running an ablation where misconception labels are included in the embedding text.

## 3. Results
We report results from a 5-fold stratified cross-validation.

### 3.1 RQ1: Narrative Fidelity as a Calibration Mechanism
Our comparison of the Main (Narrative) run and the Ablation (Label-Aware) run reveals a critical insight: asking "why" filters out noise.

**Table 1: The Reasoning Filter Effect (Opus 2 Runs)**
| Evaluation Mode | Precision | Recall | F1 Score | Specificity |
| :--- | :--- | :--- | :--- | :--- |
| **Thinking-Only (Main)** | **0.577** | 0.872 | **0.694** | **84.8%** |
| Thinking+Labels (Ablation) | 0.511 | **0.982** | 0.673 | 77.4% |

In the Ablation study, where models could rely on label keywords, Recall was near-perfect (98.2%), but Precision collapsed to 0.511. The models became "trigger-happy," diagnosing misconceptions in clean code (Specificity drop to 77.4%). Requiring the model to articulate the *student's thinking* (Main) served as a functional constraint, reducing False Positives by ~33% (from 5,616 to 3,884) and improving the overall F1 score.

### 3.2 RQ2: The Visibility Gap
Performance is not uniform. We observe a stark "Abstraction Gradient" where models struggle with misconceptions that lack syntactic markers.

**Table 2: Performance by Error Visibility**
| Category Type | Examples | Mean Recall | Difficulty |
| :--- | :--- | :--- | :--- |
| **Structural** | *Void Machine*, *Human Index* | **97.6%** | Easy |
| **Semantic** | *Reactive State*, *Fluid Type* | **74.1%** | Hard |

For example, *The Void Machine* (expecting a return value from a void function) is syntactically obvious and detected 99.4% of the time. In contrast, *The Reactive State Machine* (expecting a variable to auto-update like a spreadsheet cell) is syntactically valid but semantically flawed; detection dropped to 77.2%. This suggests LLMs are still primarily "super-linters" rather than deep cognitive modelers.

### 3.3 RQ3: Reliability via Ensembles
Individual models are noisy. Ensembling offers a path to deployment-ready reliability.
*   **Raw Performance:** F1 = 0.695
*   **Model Ensemble (Vote $\ge$ 2/6):** F1 = **0.762**

The ensemble approach yields a Precision gain of +10.5%, pushing the system closer to a usable threshold for classroom feedback.

## 4. Discussion

### 4.1 The Pedagogical Value of False Positives
A deeper analysis of the False Positives (FPs) reveals that 86.6% are `FP_CLEAN`—detections on correct code. However, qualitative review suggests many of these are "Pedagogical Over-Sensitivity."
*   *Example:* A student writes `if (x == true)`. The code is correct, but the LLM flags it as *Boolean Literal Blindness*.
*   *Implication:* While technically a False Positive, an AI tutor pointing this out is likely *helpful*. The high FP rate is partly a measurement artifact of the "Perfect Code" assumption in our control set.

### 4.2 Recommendations for ITiCSE Community
1.  **Demand "Why":** Educational tools should not just classify errors. Forcing the generation of a student narrative significantly improves diagnostic specificity.
2.  **Human-in-the-Loop:** Given the Semantic Gap (RQ2), LLMs should be used as "hypothesis generators" for instructors, not autonomous graders, especially for logic-heavy assignments.

## 5. Threats to Validity
The primary limitation is the **Synthetic Circularity**. We use an LLM (GPT-5.1) to simulate students and an LLM (GPT-5.2/Claude) to diagnose them. While we rigorously validated the dataset with a 6-step compiler/test pipeline, this measures the models' ability to understand *consistent* synthetic errors. Real students may hold fragmented or inconsistent mental models that are harder to diagnose. Thus, our results represent an upper bound on capability.

## 6. Conclusion
We demonstrated that LLMs can achieve high Narrative Fidelity (F1=0.694) in diagnosing CS1 misconceptions, provided they are constrained to articulate the student's thought process. This "reasoning requirement" acts as a vital filter against hallucination, improving specificity by 7.4%. However, the persistent gap in diagnosing semantic/invisible misconceptions highlights that we have not yet solved the "Mind" part of "Mind Over Syntax."

## References
*   du Boulay, B. (1986). Some difficulties of learning to program.
*   Sorva, J. (2013). Notional Machines and Introductory Programming Education.
*   (See full draft for complete references)

