# ITiCSE 2026 Position Paper Abstract (Revised)

Effective CS1 tutoring requires more than fixing syntax errors—it demands diagnosing the *notional machines* students hold: their latent beliefs about how programs execute. Correcting these mental models produces durable learning gains that surface-level feedback cannot. As LLM-based tutors become ubiquitous, we must ask: can they infer student beliefs from code evidence, or do they merely restate bugs with post-hoc rationales?

We argue that current evaluation paradigms—bug detection accuracy, label prediction—are insufficient for this question. Tutors should produce *belief narratives* grounded in submission evidence, and evaluations must explicitly measure false-alarm rates on correct work. Over-diagnosis is a high-impact harm: telling students they hold misconceptions they do not erodes trust and wastes instructional time.

We instantiate this position with TRACER, a label-blind evaluation framework where model-generated belief narratives are matched to reference misconception descriptions via cross-validated semantic-similarity thresholds. On a synthetic benchmark of 1200 Java CS1 submissions spanning 18 notional-machine misconceptions, current LLMs achieve high recall (0.87) but moderate precision (0.58). Critically, 87% of false positives occur on correct code—models over-diagnose clean work. A label-aware ablation boosts recall (0.98) while degrading specificity (0.77), demonstrating how evaluation shortcuts amplify over-diagnosis.

These findings motivate a research agenda for the CS education community: validating narrative-fidelity metrics on authentic student data, designing tutors that surface diagnostic uncertainty, and establishing when LLM tutors should defer to human oversight. The question is not whether LLMs can help—but whether they can help responsibly.
