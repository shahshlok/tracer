LLM-based CS1 tutors can fix code and generate fluent explanations, but effective tutoring hinges on a different capability: identifying the learner’s notional machine—their latent beliefs about how programs execute. Making these beliefs explicit allows educators to focus feedback on what students plausibly think, rather than reacting only to surface-level syntactic or logical errors. As LLM-based tutors become widespread, a critical question arises: can such systems infer student beliefs from code evidence, or do they merely rationalize observed behavior after the fact?

We argue that prevailing evaluation paradigms for LLM tutors—such as bug detection accuracy—are misaligned with this educational goal. In particular, they obscure a high-impact pedagogical harm: **over-diagnosis**, where tutors attribute misconceptions to students whose code is correct. Belief attribution is an epistemically asymmetric task: false positives are more harmful than silence, yet current evaluations rarely measure diagnostic specificity on correct work.

To support this position, we introduce TRACER, a label-blind evaluation framework that treats belief diagnosis as a narrative justification task grounded in submission evidence. Using a controlled synthetic benchmark designed to probe diagnostic behavior under known ground truth, we find that current LLMs achieve high recall in identifying misconceptions (0.87) but substantially lower precision (0.58), with most errors arising from over-diagnosis of correct code. An ablation using label-aware shortcuts further inflates recall while degrading specificity, illustrating how common evaluation practices amplify this failure mode.

These findings motivate a research agenda for CS education: developing narrative-fidelity metrics for belief diagnosis, validating diagnostic specificity on authentic student data, designing tutors that surface uncertainty, and establishing principled conditions under which LLM tutors should defer judgment.
