# ITiCSE 2026 Position Paper Abstract (Revised)

LLM-based CS1 tutors can fix code and generate fluent explanations, but effective tutoring hinges on a different capability: identifying the learner’s notional machine—their latent beliefs about how programs execute. As AI-assisted programming (“vibe coding”) spreads and tools mask syntax errors, conceptual misconceptions may well be the primary obstacle. This begs the question: Can LLM tutors infer student beliefs from code evidence, or do they mainly rephrase bugs with post-hoc rationales?

We argue that prevailing evaluations—bug labels, test passing, and explanation plausibility—are misaligned with the educational goal of understanding student thinking. Belief attribution is asymmetric: over-diagnosis on correct work wastes time and erodes trust, yet specificity on clean submissions is rarely reported. We propose evaluating tutors as hypothesis generators: they must output a belief narrative grounded in code evidence and quantify false-alarm rates.

We instantiate this position with TRACER, a label-blind evaluation that matches model belief narratives to reference misconceptions using cross-validated semantic-similarity thresholds. On a synthetic upper-bound benchmark of 1,200 Java CS1 submissions (18 misconceptions; 275 seeded, 925 clean) across six models, LLMs achieve high recall (0.87) but moderate precision (0.58), with most false positives arising from over-diagnosis of clean code (86.6% of FPs; specificity 0.85). A label-aware ablation inflates recall (0.98) while reducing specificity (0.77), illustrating how shortcuts can make systems look “more diagnostic” while becoming less safe.

We outline a research agenda for narrative-fidelity metrics, validation on authentic student data, and tutors that surface uncertainty and defer when evidence is weak.
