Explaining a bug is easy; understanding *why a student wrote the bug* is the real work of tutoring. As LLMs are increasingly applied to CS1 student code submissions, the opportunity—and risk—is not merely automated bug fixing, but whether these systems can help educators reason about the latent beliefs students hold about how programs execute. If such beliefs can be inferred reliably, instructors can target feedback at student reasoning rather than reacting only to surface-level syntactic or logical errors.

We argue that realizing this opportunity requires the CS education community to move beyond using LLMs merely to grade or label student code. Diagnosing student beliefs is an epistemically asymmetric task: falsely attributing a misconception to a student who does not hold it is more harmful than remaining silent. We propose that the community explore using LLMs as reasoning agents to infer plausible student mental models, enabling educators to target underlying beliefs rather than just surface-level syntax or logic errors. We contend that tutors should be evaluated on their ability to produce evidence-grounded student-belief narratives and on their specificity when student work is correct.

We instantiate this position with TRACER, a label-blind narrative-fidelity evaluation that assesses whether model-generated belief narratives align with reference misconception descriptions without access to labels. Using a controlled synthetic benchmark of 1,200 Java CS1 submissions spanning 18 notional-machine misconceptions, we find that current LLMs achieve high recall (0.87) but only moderate precision (0.58), with the majority of errors arising from over-diagnosis of correct code. A label-aware ablation further inflates recall (0.98) while degrading specificity (0.77), illustrating how evaluation shortcuts amplify this failure mode.

Together, these results motivate a research agenda for deeper use of LLMs in CS1 education: validating narrative-fidelity metrics on authentic student data, designing tutors that surface diagnostic uncertainty, and establishing principled conditions under which LLM tutors should defer to human instructors. The central question is not whether LLMs can assist with student code—but whether they can support educators in understanding student thinking.
