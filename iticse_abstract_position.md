LLM-based CS1 tutors can now fix code and generate plausible explanations, but effective tutoring often depends on something harder: diagnosing a learner’s *notional machine*—the latent beliefs they hold about how programs execute. Can today’s LLMs infer these student beliefs from code evidence, or do they mostly restate surface-level bugs and post-hoc rationales? We argue that answering this requires evaluation beyond “find the bug” or label prediction: tutors should be required to produce a student-belief narrative grounded in the submission, and evaluations should explicitly measure false-alarm rates on correct work (over-diagnosis is a high-impact harm in tutoring). We instantiate this position with TRACER, a label-blind narrative-fidelity evaluation where model-generated belief narratives are matched to reference misconception narratives using cross-validated semantic-similarity thresholds. In a synthetic upper-bound benchmark of 1,200 Java CS1 submissions (18 notional-machine misconceptions; 275 seeded, 925 clean), models show high recall (0.87) but only moderate precision (0.58); most errors are false alarms on clean code (≈87% of FPs), and performance varies sharply across notional-machine categories. A label-aware ablation increases recall (0.98) while worsening specificity (0.77), illustrating how label leakage can amplify over-diagnosis. We outline a research agenda for validating narrative-fidelity metrics on authentic student data and for designing tutors that surface uncertainty and defer to human oversight when evidence is weak.
