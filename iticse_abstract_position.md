# ITiCSE Position Papers and Proposals (PPP) Track Abstract

As LLMs enter CS1 tutoring, the main safety failure is not missing a bug but overconfidently attributing a misconception to a correct student. Current evaluations largely treat diagnosis as label prediction or bug finding, which misses whether the model’s explanation matches the student’s underlying notional machine. We argue for evaluating diagnostic narrative fidelity: the model should produce a student-belief narrative grounded in the program, and evaluation should explicitly measure false alarms on correct work. We operationalize this view with TRACER, which scores belief narratives against reference misconception narratives using semantic similarity with calibrated thresholds, and we show in a controlled synthetic benchmark (1,200 Java CS1 programs; 18 seeded misconceptions plus clean controls) that today’s LLMs tend toward high recall but substantial over-diagnosis, with large variation across notional machine types. We propose that tutoring systems treat diagnoses as hypotheses, surface uncertainty, and trigger human oversight when evidence is weak. We outline a research agenda for validating narrative-fidelity metrics on authentic student data and for designing interventions that reduce false-alarm harms while preserving instructional usefulness.

