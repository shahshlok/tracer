Explaining a bug is easy; understanding *why a student wrote it* is the real work of tutoring. As LLMs enter CS1 classrooms, the opportunity is not only automated fixing, but helping instructors infer students’ latent beliefs about execution (their *notional machines*). We argue the community should explore LLMs as instructor-facing **hypothesis generators**—while treating belief attribution as epistemically asymmetric: false attributions are more harmful than silence.

This requires evaluation practices that reward *diagnostic humility*: evidence-grounded narratives and calibrated abstention on correct work. We further contend that diagnosis should be reported separately for **structural** misconceptions (visible in code) versus **semantic** misconceptions (requiring intent inference), rather than collapsed into a single score.

We support this position with TRACER, a controlled testbed that evaluates whether model-generated belief narratives align with reference misconception descriptions via label-blind semantic matching. On 1,200 synthetic Java CS1 submissions spanning 18 misconceptions, LLMs achieve high recall (0.87) but only moderate precision (0.58), with most false positives arising from over-diagnosis of correct code. A label-leakage ablation in semantic matching saturates recall (0.98) while degrading specificity (0.77), showing how shortcut evaluation pipelines can inflate apparent capability while worsening safety.

We propose a research agenda: (1) make specificity/false-alarm behavior on correct code a first-class metric; (2) stratify results by structural–semantic gap; and (3) pilot instructor-facing tools under these standards on authentic student data before high-stakes deployment.
