PRELIMINARY MODEL VARIANCE INVESTIGATION
=========================================
Date: 2025-11-24
Assignment: Insurance Compute (COSC 111)
Models Tested: 4 models across multiple student submissions

MODELS ANALYZED
---------------
1. google/gemini-2.5-flash-lite (Gemini Lite)
2. openai/gpt-5-nano (GPT Nano)
3. google/gemini-2.5-flash (Gemini Flash)
4. openai/gpt-5.1 (GPT 5.1)


SCORING PATTERNS OBSERVED
--------------------------

Model: google/gemini-2.5-flash-lite
  - Average Scores: 85-86% range (6.0/7.0)
  - Misconceptions Found: 2 per submission
  - Grading Style: Critical but fair
  - Key Behaviors:
    * Deducts for suboptimal implementations (e.g., basic email validation)
    * Identifies style issues (e.g., brittle input handling)
    * Gives partial credit when logic is present but flawed
    * Focuses on robustness and real-world applicability

Model: openai/gpt-5-nano
  - Average Scores: 100% (7.0/7.0) - both submissions
  - Misconceptions Found: 0 consistently
  - Grading Style: Very lenient, literal rubric interpretation
  - Key Behaviors:
    * Accepts minimal requirements as sufficient
    * "If it works for basic test cases, it's correct"
    * Does not penalize for suboptimal patterns
    * Reserves misconceptions for obvious failures only
    * Focuses on whether code meets literal rubric requirements

Model: google/gemini-2.5-flash
  - Average Scores: 57-64% range (4.0-4.5/7.0)
  - Misconceptions Found: 2-3 per submission
  - Grading Style: Very strict, compilation-focused
  - Key Behaviors:
    * Heavily penalizes syntax/compilation errors
    * Catches assignment vs equality errors (age = 18 vs age == 18)
    * Identifies missing imports and structural issues
    * Awards 0 points for categories with compilation errors
    * Most critical of all models tested

Model: openai/gpt-5.1
  - Average Scores: 78-82% range (5.5-5.75/7.0)
  - Misconceptions Found: 2 per submission
  - Grading Style: Moderate/balanced
  - Key Behaviors:
    * Catches compilation errors but gives partial credit
    * Distinguishes between logic intent and syntax issues
    * Awards points for correct conceptual approach despite errors
    * More forgiving than Gemini Flash, stricter than GPT Nano
    * Balances literal requirements with software engineering practices


MISCONCEPTION DETECTION PATTERNS
---------------------------------

Consistent Across Models:
  - Compilation errors are flagged by both Gemini Flash and GPT 5.1
  - Missing imports, syntax errors caught by stricter models

Model-Specific Patterns:

GPT-5 Nano:
  - Threshold appears to be: "Does it fail completely?"
  - Only flags misconceptions for total breakdowns
  - Zero misconceptions in 2/2 samples despite code having issues

Gemini Flash Lite:
  - Flags suboptimal patterns even when functional
  - Identifies style and robustness concerns as misconceptions
  - Examples: "Basic String Validation", "Poor Input Handling Strategy"

Gemini Flash:
  - Most aggressive misconception detection
  - Flags syntax errors, logic errors, and style issues
  - Examples: "Missing Import", "Confusion between Assignment and Equality"

GPT 5.1:
  - Moderate misconception detection
  - Focuses on conceptual gaps and compilation blockers
  - Examples: "Incorrect translation of eligibility rule"


BLOOM'S LEVEL USAGE
-------------------
All models now correctly associate misconceptions with Bloom's levels:
  - Understand: Basic syntax, imports (e.g., missing Scanner import)
  - Apply: Conditional logic errors (e.g., age = 18 vs age < 18)
  - Analyze: Calculation logic errors (e.g., incorrect surcharge base)
  - Evaluate: Style and structural issues (e.g., convoluted control flow)


GRADING SEVERITY RANKING
-------------------------
Most Strict  â†’ Most Lenient:
1. google/gemini-2.5-flash (57-64% avg)
2. openai/gpt-5.1 (78-82% avg)
3. google/gemini-2.5-flash-lite (85-86% avg)
4. openai/gpt-5-nano (100% avg)


SAMPLE COMPARISON: Campbell_Laura_100052
-----------------------------------------
Student Issue: Used "age = 18" (assignment) instead of "age < 18" (comparison)
              Missing Scanner import, lowercase "string" type

Gemini Flash Score: 57.1% (4.0/7.0)
  - Gave 0.0/1.0 for Rule 1 checking (compilation error)
  - Gave 0.0/1.5 for email validation (compilation error)
  - Found 2 misconceptions

GPT 5.1 Score: 78.6% (5.5/7.0)
  - Gave 0.0/1.0 for Rule 1 checking (same compilation error)
  - Gave 1.5/1.5 for email validation (logic is correct despite error)
  - Found 2 misconceptions

Key Difference: GPT 5.1 gives credit for correct conceptual logic even when
                compilation fails, while Gemini Flash does not.


SAMPLE COMPARISON: Brooks_Caleb_100051
---------------------------------------
Student Issue: Incorrect surcharge calculation (25% of 600 instead of total premium)
              Missing closing brace, lowercase "string" type

Gemini Flash Score: 64.3% (4.5/7.0)
  - Gave 0.5/1.5 for cost calculation (recognized conceptual error)
  - Gave 0.0/1.5 for email validation (compilation error)
  - Found 3 misconceptions

GPT 5.1 Score: 82.1% (5.75/7.0)
  - Gave 0.75/1.5 for cost calculation (partial credit for mostly correct)
  - Gave 1.5/1.5 for email validation (logic is correct)
  - Found 2 misconceptions

Key Difference: GPT 5.1 more generous with partial credit for "mostly right" logic


IMPLICATIONS FOR ENSEMBLE GRADING
----------------------------------

Observed Variance Benefits:
  - Multiple perspectives on same code
  - Captures range from "does it work?" to "is it correct?"
  - Gemini Flash catches all compilation errors
  - GPT Nano provides baseline "minimum acceptable" bar
  - GPT 5.1 provides balanced middle ground

Challenges:
  - 40+ percentage point spread between strictest and most lenient
  - Inconsistent misconception detection
  - Different philosophies on partial credit
  - GPT Nano's consistent 100% scores reduce ensemble diversity

Aggregation Strategies to Consider:
  1. Median score (to avoid extremes)
  2. Weighted average (weight more balanced models higher)
  3. Misconception union (flag if ANY model finds it)
  4. Misconception intersection (flag if MULTIPLE models agree)
  5. Category-level consensus (require 2+ models to agree on score)


NEXT STEPS
----------
1. Evaluate more submissions to confirm patterns
2. Test with additional models for more diversity
3. Analyze misconception overlap between models
4. Develop aggregation algorithm for ensemble scores
5. Consider excluding or down-weighting GPT-5 Nano due to leniency bias
6. Investigate if Gemini Flash's strictness is too harsh or appropriately rigorous
7. Validate against human TA grades (ground truth)


QUESTIONS TO INVESTIGATE
-------------------------
- Is GPT-5 Nano's leniency consistent across all assignment types?
- Does Gemini Flash's strictness improve student learning outcomes?
- What's the "correct" balance between syntax and logic in intro CS grading?
- Should compilation errors result in 0 points or partial credit?
- How do we weight misconceptions by Bloom's level in ensemble analysis?
